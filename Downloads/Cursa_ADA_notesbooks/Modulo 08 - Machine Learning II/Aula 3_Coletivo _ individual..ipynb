{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ed78b7",
   "metadata": {},
   "source": [
    "# Ensemble: Bagging x Boosting x Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cd92ee",
   "metadata": {},
   "source": [
    "Ensemble nada mais é do que a sabedoria da maioria. Aqui combinamos vários modelos mais simples em um único modelo robusto a fim de reduzir o viés, variância e/ou aumentar a acurácia.\n",
    "<br>\n",
    "\n",
    "\n",
    "## Tipos de Ensemble:\n",
    "- __1. Bagging (short for bootstrap aggregation)__: Treina paralelamente N modelos mais fracos (geralmente do mesmo tipo - homogênio) com N subsets distintos criados com amostragem randômica e reposição. Cada modelo é avaliado na fase de teste com o label definido pela moda (classificação) ou pela média dos valores (regressão). Os métodos de Bagging reduzem a variância da predição. <br>\n",
    "Algoritimos  famosos: Random Forest <br>\n",
    "<img src='images/bagging.png' style=\"width:600px\"  text=\"http://cheatsheets.aqeel-anwar.com\" />  \n",
    "<br>\n",
    "<br>\n",
    "- __2. Boosting__: Treina N modelos mais fracos (geralmente do mesmo tipo - homogênio) de forma sequencial. Os pontos que foram classificados erroneamente recebem um peso maior para entrar no próximo modelo. Na fase de teste, cada modelo é avaliado com base do erro de teste de cada modelo, a predição é feita com um peso sobre a votação. Os métodos de Boosting reduzem o viés da predição. <br>\n",
    "Algoritimos  famosos: AdaBoost, Gradient Boosting, XGBoost, CatBoost, LightGBM (Light Gradient Boosting Machine) <br>\n",
    "<img src='images/boosting.png' style=\"width:600px\" text=\"Fonte: http://cheatsheets.aqeel-anwar.com\" />\n",
    "<br>\n",
    "<br>\n",
    "- __3. Stacking__: Treina N modelos mais fracos (geralmente de tipos distintos - heterogênio) em um subset do conjunto de dados. Uma vez que os modelos foram treinados, cria-se um novo modelo (meta learning) para combinar a saída de cada um dos modelos mais fracos resultando na predição final. Isso é feito no segundo subset dos dados. Na fase de teste, cada modelo mais fraco faz sua predição independentemente e esses labels entram como features do meta learner para gerar a predição final.\n",
    "<br>\n",
    "<img src='images/stacking.png' style=\"width:600px\" text=\"Fonte: http://cheatsheets.aqeel-anwar.com\" />\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "##### Resumo:\n",
    "<img src='images/comparison_img.png' style=\"width:600px\" text=\"Fonte: https://quantdare.com/what-is-the-difference-between-bagging-and-boosting\" />\n",
    "\n",
    "<img src='images/comparison.png' style=\"width:600px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb040d8",
   "metadata": {},
   "source": [
    "# Boosting : AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f96e0",
   "metadata": {},
   "source": [
    "O AdaBoost significa **Adaptive Boosting**, e tem como procedimento geral **a criação sucessiva dos chamados weak learners**, que são modelos bem fracos de aprendizagem - geralmente, **árvores de um único nó (stumps)**.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1744/1*nJ5VrsiS1yaOR77d4h8gyw.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5db87c",
   "metadata": {},
   "source": [
    "O AdaBoost utiliza os **erros da árvore anterior para melhorar a próxima árvore**. As predições finais são feitas com base **nos pesos de cada stump**, cuja determinação faz parte do algoritmo!\n",
    "\n",
    "<img src=\"https://static.packt-cdn.com/products/9781788295758/graphics/image_04_046-1.png\" width=700>\n",
    "\n",
    "Vamos entender um pouco melhor...\n",
    "\n",
    "Aqui, o bootstrapping não é utilizado: o método começa treinando um classificador fraco **no dataset original**, e depois treina diversas cópias adicionais do classificador **no mesmo dataset**, mas dando **um peso maior às observações que foram classificadas erroneamente** (ou, no caso de regressões, a observações **com o maior erro**).\n",
    "\n",
    "Assim, após diversas iterações, classificadores/regressores vão sequencialmente \"focando nos casos mais difíceis\", e construindo um classificador encadeado que seja forte, apesar de utilizar diversos classificadores fracos em como elementos fundamentais.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Zhuo_Wang8/publication/288699540/figure/fig9/AS:668373486686246@1536364065786/Illustration-of-AdaBoost-algorithm-for-creating-a-strong-classifier-based-on-multiple.png\" width=500>\n",
    "\n",
    "De forma resumida, as principais ideias por trás deste algoritmo são:\n",
    "\n",
    "- O algoritmo cria e combina um conjunto de **modelos fracos** (em geral, stumps);\n",
    "- Cada stump é criado **levando em consideração os erros do stump anterior**;\n",
    "- Alguns dos stumps têm **maior peso de decisão** do que outros na predição final;\n",
    "\n",
    "As classes no sklearn são:\n",
    "\n",
    "- [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "\n",
    "- [AdaBoostRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor)\n",
    "\n",
    "Note que não há muitos hiperparâmetros. O mais importante, que deve ser tunado com o grid/random search, é:\n",
    "\n",
    "- `n_estimators` : o número de weak learners encadeados;\n",
    "\n",
    "Além disso, pode também ser interessante tunar os hiperparâmetros dos weak learners. Isso é possível de ser feito, como veremos a seguir!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c7f60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T00:55:12.207316Z",
     "start_time": "2022-06-03T00:55:12.190322Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e1bfe",
   "metadata": {},
   "source": [
    "# Voltando para nosso exemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e5fa8",
   "metadata": {},
   "source": [
    "Usando n_estimator = 150 - estou chutando um valor. O ideal é fazer um grid/randomsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d6d45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T00:58:06.677561Z",
     "start_time": "2022-06-03T00:58:06.103584Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# pipeline final\n",
    "# n_estimators=50\n",
    "# Treinar o modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ddfa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T00:58:14.683093Z",
     "start_time": "2022-06-03T00:58:14.086442Z"
    }
   },
   "outputs": [],
   "source": [
    "# avalia o modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazer um gridsearch para melhorar os dados\n",
    "# fazer um adaboost para regressão"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
